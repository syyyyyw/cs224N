 顺序为video,slides,notes  
![1-1](https://github.com/syyyyyw/cs224N/blob/master/image/1-1.png)
![1-2](https://github.com/syyyyyw/cs224N/blob/master/image/1-2.png)
![1-3](https://github.com/syyyyyw/cs224N/blob/master/image/1-3.png)
![1-4](https://github.com/syyyyyw/cs224N/blob/master/image/1-4.png)
![1-5](https://github.com/syyyyyw/cs224N/blob/master/image/1-5.png)
![1-6](https://github.com/syyyyyw/cs224N/blob/master/image/1-6.png)
![1-7](https://github.com/syyyyyw/cs224N/blob/master/image/1-7.png)
![1-8](https://github.com/syyyyyw/cs224N/blob/master/image/1-8.png)  
word2vec模型训练有两种形式，分别为SG,CBOW。    
SG全称为skip-grams,由中心词center去预测两侧的词  
CBOW全称为Continuous bags of words,与SG相反  
### notes
svd 奇异值分解
word-ducument matrix->Window based Co-occurrence Matrix  
|v|\*m->|v|\*|v|  
word2vec  
two methods: negative sampling and hierarchical softmax. 
CBOW具体求导怎么做的
朴素贝叶斯假设？
